# -*- coding: utf-8 -*-
"""10_optimized_model_deployment_on_unseen_test_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_k5I1vtde9KYv_WEzs9kWKpgpABwpkvc

# BANK CUSTOMER CHURN PREDICTION - ENSEMBLE EXTREME GRADIENT BOOSTING (XGBOOST) CLASSIFIER - FINALIZED OPTIMIZED MODEL DEPLOYMENT ON UNSEEN TEST DATA

## 1. IMPORTING THE PYTHON LIBRARIES
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import math

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from xgboost import XGBClassifier
from sklearn.tree import export_graphviz, export_text
from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score

import graphviz
import pydotplus

import pickle

print("Python Libraries Import Completed")

from google.colab import drive
drive.mount('/content/drive')

"""## 2. LOADING THE RAW DATA FROM A CSV FILE"""

actual_raw_data = pd.read_csv("/content/drive/MyDrive/~~~VP_Data_Science/DS_Real_Time_Projects/Bank_Customers_Churn_Prediction_Exploring_7_Different_Classification_Algorithms/data/Bank_Churn_Unseen_Test_Data.csv")

print("Raw Data Import Completed")

"""## 3. DATA EXPLORATION"""

# Verifying the shape of the data

actual_raw_data.shape

# Displaying the first 5 Rows of Data Instances

actual_raw_data.head()

# Displaying the last 5 Rows of Data Instances

actual_raw_data.tail()

# Verifying the Column Names in the Raw Data

actual_raw_data.columns

# Verifying the Type of the Columns in the Raw Data

actual_raw_data.dtypes

# Verifying the Null Values in the Raw Data

actual_raw_data.isnull().sum()

"""## 4. DATA VISUALISATION"""

# Creating a New Data Frame To Include Only the Relevant Input Independent Variables and the Output Dependent Variable

raw_data = actual_raw_data[['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure',
                           'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',
                           'EstimatedSalary', 'Exited']]

raw_data

"""## 5. DATA PRE-PROCESSING"""

# Converting the Categorical Variables into Numeric One-Hot Encoded Variables for Decision Tree IDE Model Training Purposes

raw_data_pp = pd.get_dummies(raw_data, columns=['Geography', 'Gender', 'HasCrCard', 'IsActiveMember'])

print("Execution Completed")

# Verifying the Columns of the Pre-processed Raw Data Frame after Applying One-Hot Encoding Method

raw_data_pp.head()

# Verifying the Shape of the Pre-processed Raw Data Frame after Applying One-Hot Encoding Method

raw_data_pp.shape

# Normalising the Continuous Variables Columns to Scale to a Value Between 0 and 1 for Decision Tree IDE Model Training Purposes

norm_scale_features = ['CreditScore', 'Age', 'Balance','EstimatedSalary']

norm_scale = MinMaxScaler()

raw_data_pp[norm_scale_features] = norm_scale.fit_transform(raw_data_pp[norm_scale_features])

print("Scaling is Completed")

# Verifying all the Columns of the Final Pre-processed Raw Data Frame after Applying the Scaling Method

raw_data_pp.head()

# Verifying the Shape of the Pre-processed Raw Data Frame after Applying the Scaling Method

raw_data_pp.shape

"""## 6. DATA SPLIT AS TEST INPUT DATA AND TEST OUTPUT DATA"""

# Defining the Input and the Target Vectors for XGBoost Model Testing Purposes

# Input (Independent) Features/Attributes
X = raw_data_pp.drop('Exited', axis=1).values

# Output (Dependent) Target Attribute
y = raw_data_pp['Exited'].values

print("Execution Completed")

# Verifying the Shape of the Test Input Data and the Test Output Data

print("Input Test: {}".format(X.shape))
print("Output Test: {}\n".format(y.shape))

"""## 7. FITTING THE FINALIZED ENSEMBLE - EXTREME GRADIENT BOOSTING (XGBOOST) CLASSIFIER MODEL WITH THE DEFAULT PARAMETERS VALUES"""

# Defining the Instance of XGBoost Classifier Final Model with the Best Parameters
xgboost_final_model = XGBClassifier(colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=6,
                                    tree_method='hist')

print("Finalized Model Training Started.....")

# Fitting and the Final XGBoost Classifier Model based on the Best Parameters Selected
xgboost_final_model.fit(X, y)

print("Finalized Model Training Completed.....")

"""## 8. SAVING AND LOADING THE FITTED AND PRE-TRAINED MODEL"""

# Saving the Fitted and Pre-trained XGBoost Model using Pickle

model_save_file = "xgboost_final_model.sav"

model_save = pickle.dump(xgboost_final_model, open(model_save_file, 'wb'))

print("Model is Saved to the Local Directory Successfully")

# Loading the Pre-trained XGBoost Model using Pickle

model_path = "/content/drive/MyDrive/~~~VP_Data_Science/DS_Real_Time_Projects/Bank_Customers_Churn_Prediction_Exploring_7_Different_Classification_Algorithms/model/xgboost_final_model.sav"

model_load = pickle.load(open(model_path, 'rb'))

print("Pre-trained Model is Loaded from the Local Directory Successfully")

"""## 9. TRAINING VERSUS VALIDATION ACCURACY"""

# Accuracy on the Unseen Test Data

print("Training Accuracy: ", model_load.score(X, y))
#print("Training Accuracy: ", xgboost_final_model.score(X, y))

"""## 10. EVALUATING THE CLASSIFIER RESULTS ON THE UNSEEN TEST DATA"""

# Evaluating the Classifier Results on the Unseen Test Data

y_test_pred_final = model_load.predict(X)

#y_test_pred_final = xgboost_final_model.predict(X)

print(y_test_pred_final)

# Evaluating the Classifier Probability Results on the Unseen Test Data

y_test_pred_prob_final = model_load.predict_proba(X)

#y_test_pred_prob_final = xgboost_final_model.predict_proba(X)

print(y_test_pred_prob_final)

"""## 11. COMPARING THE UNSEEN TEST PREDICTIONS WITH THE UNSEEN TEST ACTUALS"""

# Comparing the Unseen Test Predictions with the Unseen Test Actuals for the first 20 Data Instances

# Unseen Test Actuals
print(y[:20])

# Unseen Test Predictions
print(y_test_pred_final[:20])

# Comparing the Unseen Test Probability Predictions with the Unseen Test Actuals for the first 20 Data Instances

# Probability Value of Less Than 0.5 Represents Class 0 (No Churn)
# Probability Value of Greater Than 0.5 Represents Class 1 (Churn)

# Unseen Test Actuals
print(y[:20])

# Unseen Test Predictions
print(y_test_pred_prob_final[:20])

# Creating a Function to Select the Second Probability Value (Representing Class 1 "Churn" Probability)

def churn_prob_pred_selection(array, i):
  return [prob[i] for prob in array]
    
# calling the Function
churn_prob_pred_selection(y_test_pred_prob_final, 1)

"""## 12. CONFUSION MATRIX BETWEEN THE UNSEEN TEST ACTUALS AND THE UNSEEN TEST PREDICTIONS"""

# Defining the Instance of Confusion Matrix
cm_validation_matrix_final = confusion_matrix(y, y_test_pred_final)

print("Execution Completed")

"""### Method 1 : Plotting the Confusion Matrix with Numeric Values using Seaborn heatmap() Function"""

# Method 1 : Plotting the Confusion Matrix with Numeric Values using Seaborn heatmap() Function

churn_cm_plot_1 = sns.heatmap(cm_validation_matrix_final, annot=True)
churn_cm_plot_1

"""### Method 2 : Plotting the Confusion Matrix with Percentage Values using Seaborn heatmap() Function"""

# Method 2 : Plotting the Confusion Matrix with Percentage Values Rounded-off to 2 Decimal Places using Seaborn heatmap() Function

churn_cm_plot_2 = sns.heatmap(cm_validation_matrix_final/np.sum(cm_validation_matrix_final), annot=True, fmt='0.2%', cmap='plasma')
churn_cm_plot_2

"""### Method 3 : Plotting the Confusion Matrix with Numeric Values, Percentage Values and the Corresponding Text using Seaborn heatmap() Function"""

# Method 3 : Plotting the Confusion Matrix with Numeric Values, Percentage Values and the Corresponding Text using Seaborn heatmap() Function

cm_names_final = ['True Negative', 'False Positive', 'False Negative', 'True Positive']

cm_counts_final = ["{0:0.0f}".format(value) for value in cm_validation_matrix_final.flatten()]

cm_percentages_final = ["{0:0.2%}".format(value) for value in cm_validation_matrix_final.flatten()/np.sum(cm_validation_matrix_final)]

cm_labels_final = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in zip(cm_names_final,cm_counts_final,cm_percentages_final)]

cm_labels_final = np.asarray(cm_labels_final).reshape(2,2)

sns.heatmap(cm_validation_matrix_final, annot=cm_labels_final, fmt='', cmap='jet')

"""## 13. CLASSIFICATION REPORT BETWEEN THE UNSEEN TEST ACTUALS AND THE UNSEEN TEST PREDICTIONS"""

# Classification Report and Metrics between the Unseen Test Actuals and the Unseen Test Predictions

target_names = ['No Churn', 'Churn']

# Defining the Classification Report for the Validation Data
classification_report_test_final = classification_report(y, y_test_pred_final, target_names=target_names)

# Displaying the Classification Report
print(classification_report_test_final)

"""## 14. INDIVIDUAL CLASSIFIER METRICS BETWEEN THE UNSEEN TEST ACTUALS AND THE UNSEEN TEST PREDICTIONS"""

# Individual Classifier Metrics between the Unseen Test Actuals and the Unseen Test Predictions

# Accuracy
churn_accuracy = round((accuracy_score(y, y_test_pred_final))*100, 2)

# F1-score
churn_f1_score = round((f1_score(y, y_test_pred_final)*100), 2)

# Precision
churn_precision = round((precision_score(y, y_test_pred_final)*100), 2)

# Recall
churn_recall = round((recall_score(y, y_test_pred_final)*100), 2)

# ROC AUC Score
churn_roc_auc_score = round((roc_auc_score(y, y_test_pred_final)*100), 2)

print("Customer Churn Finalized Classifier - Accuracy: {}%".format(churn_accuracy))
print("Customer Churn Finalized Classifier - F1-Score: {}%".format(churn_f1_score))
print("Customer Churn Finalized Classifier - Precision: {}%".format(churn_precision))
print("Customer Churn Finalized Classifier - Recall: {}%".format(churn_recall))
print("Customer Churn Finalized Classifier - ROC AUC Score: {}%".format(churn_roc_auc_score))

"""## 15. SAVING THE OUTPUT PREDICTIONS TO THE EXISTING RAW TEST DATA AS A DATAFRAME

### Creating a Copy of the Actual Raw Test Data DataFrame
"""

# Creating a Duplicate Copy of the Actual Raw Test Data Pandas DataFrame
output = actual_raw_data.copy()

# Verifying the Type and Shape of the Output
print("Type of the Output: ", type(output))
print("Shape of the Output: ", output.shape)

# Displaying the first 5 Data Instances in the Output Dataframe
output.head()

"""## Adding New Columns to the Output Dataframe to Update the Output Test Predictions"""

# Adding New Columns to the Output Dataframe to Update the Output Test Predictions

output['Predictions_Churn_NoChurn'] = y_test_pred_final
output['Predictions_Probability_Churn_NoChurn'] = churn_prob_pred_selection(y_test_pred_prob_final, 1)
output['Prediction_Churn_Status'] = 'empty'
output['Prediction_Churn_Status'] = np.where(output['Predictions_Churn_NoChurn'] == 0, 'Retention', 'Churn')

# Displaying the first 5 Data Instances in the Output Dataframe after Adding the New Prediction Columns
output.head()

"""## Exporting and Saving the Final Output Dataframe into a CSV File"""

# Exporting and Saving the Final Output Dataframe into a CSV File

output.to_csv("/content/drive/MyDrive/~~~VP_Data_Science/DS_Real_Time_Projects/Bank_Customers_Churn_Prediction_Exploring_7_Different_Classification_Algorithms/output/9_final_model_output_unseen_test_predictions/final_model_output_unseen_test_predictions.csv")

print("Output Predictions Data Export Completed")

"""## 16. CONCLUSION

### As we can see from the above unseen test performance results; the Ensemble - Extreme Gradient Boosting (XGBoost) Classifier Model tuned with the below parameters as 
> ### colsample_bytree = 0.5
> ### gamma = 0.1
> ### learning_rate = 0.05
> ### max_depth = 6
> ### tree_method = 'hist'

### is the final optimized model with the test accuracy of about 90%.
"""